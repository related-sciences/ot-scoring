{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Test Dataset Notebook\n",
    "\n",
    "This notebook should be used to generate and/or debug datasets for unit tests, stored as resources on the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "input_dir = /home/eczech/data/ot/extract\n",
       "input_file = /home/eczech/data/ot/extract/evidence.json\n",
       "df = [access_level: string, disease: struct<biosample: struct<id: string, name: string>, efo_info: struct<efo_id: string, label: string ... 2 more fields> ... 3 more fields> ... 12 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[access_level: string, disease: struct<biosample: struct<id: string, name: string>, efo_info: struct<efo_id: string, label: string ... 2 more fields> ... 3 more fields> ... 12 more fields]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.nio.file.Paths\n",
    "val input_dir = Paths.get(System.getProperty(\"user.home\"), \"data\", \"ot\", \"extract\")\n",
    "val input_file = input_dir.resolve(\"evidence.json\")\n",
    "val df = spark.read.json(input_file.toString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|               type| count|\n",
      "+-------------------+------+\n",
      "|   affected_pathway| 87104|\n",
      "|     rna_expression|204229|\n",
      "|       animal_model|500683|\n",
      "|   somatic_mutation| 70554|\n",
      "|         known_drug|383122|\n",
      "|genetic_association|380852|\n",
      "+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"type\").groupBy(\"type\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+------------+-------------------+----------+--------------+----------------+\n",
      "|             id|affected_pathway|animal_model|genetic_association|known_drug|rna_expression|somatic_mutation|\n",
      "+---------------+----------------+------------+-------------------+----------+--------------+----------------+\n",
      "|ENSG00000141510|             870|        1308|                587|      null|            17|            3926|\n",
      "|ENSG00000112715|              17|         433|                253|      2047|            30|            null|\n",
      "|ENSG00000105397|              20|          58|                117|        97|             1|            null|\n",
      "|ENSG00000244734|               6|        null|                355|         5|            81|            null|\n",
      "|ENSG00000169174|               6|          35|                169|       112|            20|            null|\n",
      "+---------------+----------------+------------+-------------------+----------+--------------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "genes = List(ENSG00000141510, ENSG00000244734, ENSG00000169174, ENSG00000112715, ENSG00000105397)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "List(ENSG00000141510, ENSG00000244734, ENSG00000169174, ENSG00000112715, ENSG00000105397)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val genes = List(\n",
    "    \"ENSG00000141510\", // TP53\n",
    "    \"ENSG00000244734\", // HBB\n",
    "    \"ENSG00000169174\", // PCSK9\n",
    "    \"ENSG00000112715\", // VEGFA\n",
    "    \"ENSG00000105397\"  // TYK2\n",
    ")\n",
    "import org.apache.spark.sql.functions.count\n",
    "df.filter($\"target.id\".isin(genes: _*))\n",
    "    .groupBy(\"target.id\").pivot(\"type\")\n",
    "    .agg(count(\"id\"))\n",
    "    .show(100, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Output Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "repoDir = /home/eczech/repos/ot-scoring\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "/home/eczech/repos/ot-scoring"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var repoDir = Paths.get(System.getProperty(\"user.home\"), \"repos\", \"ot-scoring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2129\n",
      "+---------------+-----------+-----+---------+------------+\n",
      "|      target_id| disease_id|score|is_direct|  source_ids|\n",
      "+---------------+-----------+-----+---------+------------+\n",
      "|ENSG00000105397|EFO_0000095|  0.1|     true|    [chembl]|\n",
      "|ENSG00000105397|EFO_0000096|  0.1|    false|    [chembl]|\n",
      "|ENSG00000105397|EFO_0000181|  0.5|     true|[slapenrich]|\n",
      "+---------------+-----------+-----+---------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "path = /home/eczech/repos/ot-scoring/target/scala-2.11/test-classes/pipeline_test/output/score_association.parquet\n",
       "df = [target_id: string, disease_id: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[target_id: string, disease_id: string ... 3 more fields]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var path = repoDir.resolve(\"target/scala-2.11/test-classes/pipeline_test/output/score_association.parquet\")\n",
    "var df = spark.read.parquet(path.toString)\n",
    "println(df.count())\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2965\n",
      "+---------------+-----------+----------+---------+---------+-----+\n",
      "|      target_id| disease_id| source_id|score_raw|is_direct|score|\n",
      "+---------------+-----------+----------+---------+---------+-----+\n",
      "|ENSG00000105397|EFO_0000095|    chembl|      0.1|     true|  0.1|\n",
      "|ENSG00000105397|EFO_0000096|    chembl|      0.1|    false|  0.1|\n",
      "|ENSG00000105397|EFO_0000181|slapenrich|      0.5|     true|  0.5|\n",
      "+---------------+-----------+----------+---------+---------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "path = /home/eczech/repos/ot-scoring/target/scala-2.11/test-classes/pipeline_test/output/score_source.parquet\n",
       "df = [target_id: string, disease_id: string ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[target_id: string, disease_id: string ... 4 more fields]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var path = repoDir.resolve(\"target/scala-2.11/test-classes/pipeline_test/output/score_source.parquet\")\n",
    "var df = spark.read.parquet(path.toString)\n",
    "println(df.count())\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2129\n",
      "+--------------+--------------------+---------------+\n",
      "|    disease_id|               score|      target_id|\n",
      "+--------------+--------------------+---------------+\n",
      "|   EFO_0003867|1.338723084160192...|ENSG00000169174|\n",
      "|   EFO_0003843| 2.09254289568363E-5|ENSG00000169174|\n",
      "|Orphanet_79211|  1.0003361559218569|ENSG00000169174|\n",
      "+--------------+--------------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "path = /home/eczech/repos/ot-scoring/src/test/resources/pipeline_test/input/association_scores.json\n",
       "df = [disease_id: string, score: double ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[disease_id: string, score: double ... 1 more field]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var path = repoDir.resolve(\"src/test/resources/pipeline_test/input/association_scores.json\")\n",
    "var df = spark.read.json(path.toString)\n",
    "println(df.count())\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2965\n",
      "+--------------+--------------------+--------------+---------------+\n",
      "|    disease_id|               score|     source_id|      target_id|\n",
      "+--------------+--------------------+--------------+---------------+\n",
      "|   EFO_0003867|1.338723084160192...|phewas_catalog|ENSG00000169174|\n",
      "|   EFO_0003843| 2.09254289568363E-5|phewas_catalog|ENSG00000169174|\n",
      "|Orphanet_79211|                 1.0|        chembl|ENSG00000169174|\n",
      "+--------------+--------------------+--------------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "path = /home/eczech/repos/ot-scoring/src/test/resources/pipeline_test/input/source_scores.json\n",
       "df = [disease_id: string, score: double ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[disease_id: string, score: double ... 2 more fields]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var path = repoDir.resolve(\"src/test/resources/pipeline_test/input/source_scores.json\")\n",
    "var df = spark.read.json(path.toString)\n",
    "println(df.count())\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Full Pipeline Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+-------------------+---------------+------------------+--------------+------------+-------------------+---+-------------------+\n",
      "|                  id|      source_id|terminal_disease_id|      target_id|    score_resource|    disease_id|is_direct_id|       score_source|rid|              score|\n",
      "+--------------------+---------------+-------------------+---------------+------------------+--------------+------------+-------------------+---+-------------------+\n",
      "|ae12a8d819d094399...|uniprot_somatic|        EFO_0000640|ENSG00000105976|               1.0|   EFO_0003086|       false|                1.0|  1|                1.0|\n",
      "|fa5ac4412ee2b1d03...|      phenodigm|        Orphanet_28|ENSG00000146085|0.9945999999999999|Orphanet_79062|       false|0.19891999999999999|  1|0.19891999999999999|\n",
      "|5a74c8828948c1c41...|        uniprot|       Orphanet_510|ENSG00000165704|               0.0|Orphanet_71859|       false|                0.0|  1|                0.0|\n",
      "+--------------------+---------------+-------------------+---------------+------------------+--------------+------------+-------------------+---+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "path = /home/eczech/repos/ot-scoring/target/scala-2.11/test-classes/pipeline_test/output\n",
       "df = [id: string, source_id: string ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[id: string, source_id: string ... 8 more fields]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var path = \"/home/eczech/repos/ot-scoring/target/scala-2.11/test-classes/pipeline_test/output\"\n",
    "//var df = spark.read.parquet(path + \"/score_source.parquet\")\n",
    "var df = spark.read.parquet(path + \"/score_evidence.parquet\")\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- source_id: string (nullable = true)\n",
      " |-- terminal_disease_id: string (nullable = true)\n",
      " |-- target_id: string (nullable = true)\n",
      " |-- score_resource: double (nullable = true)\n",
      " |-- disease_id: string (nullable = true)\n",
      " |-- is_direct_id: boolean (nullable = true)\n",
      " |-- score_source: double (nullable = true)\n",
      " |-- rid: integer (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 0 in stage 11.0 failed 1 times, most recent failure: Lost task 0.0 in stage 11.0 (TID 13, localhost, executor driver): java.io.FileNotFoundException: File file:/home/eczech/repos/ot-scoring/target/scala-2.11/test-classes/pipeline_test/output/score_evidence.parquet/part-00027-cb4cba73-7949-425a-a907-84cf2135e731-c000.snappy.parquet does not exist\n",
       "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n",
       "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n",
       "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n",
       "  at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
       "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n",
       "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n",
       "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n",
       "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n",
       "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n",
       "  at org.apache.spark.sql.Dataset.show(Dataset.scala:745)\n",
       "  at org.apache.spark.sql.Dataset.show(Dataset.scala:704)\n",
       "  ... 42 elided\n",
       "Caused by: java.io.FileNotFoundException: File file:/home/eczech/repos/ot-scoring/target/scala-2.11/test-classes/pipeline_test/output/score_evidence.parquet/part-00027-cb4cba73-7949-425a-a907-84cf2135e731-c000.snappy.parquet does not exist\n",
       "It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n",
       "  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)\n",
       "  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n",
       "  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n",
       "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n",
       "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
       "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
       "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
       "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
       "  at org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n",
       "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter($\"source_id\" === \"phenodigm\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
